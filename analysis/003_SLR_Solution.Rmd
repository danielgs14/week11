---
title: "Simple Linear Regression Exercise"
author: "Dr. Luis Malpica, based on ideas from a stats in R course by Dr. Brett Favaro"
date: "April, 2020"
output:
  pdf_document:
    toc: yes
  html_document:
    theme: united
    toc: yes
editor_options: 
  chunk_output_type: console
---

# Introduction
We will run a simple linear regression model on a simple dataset ('data/grazing.csv'), it contains data on seed production of a plant. Hypothesis is that the seed production (# of seeds) is based on the width of the root of the plant.

Main variables are:
- Seeds = # of seeds produced by the plant, this will be our response (Y) variable
- Root = width of the plant root, this will be our predictive (X) variable

# Install required packages & R custom made functions
```{r}
# If you don't yet have the following packages run:
# install.packages("tidyverse")
library(tidyverse)

# Load custom R functions
source("./R/6003Functions.R")

```

# Load & clean up data steps
```{r}
# Read in data
mydata <- read.table("data/grazing.csv", header=TRUE, sep=",")

# Remember the clean up data steps
# STEP 1: Did it load correctly?
# STEP 2: Check data types
# STEP 3: Check for missing or impossible values
# STEP 4: Check for typos and broken factors

# Skip data verification because dataset is very simple
head(mydata)

# y =  Seed = dead birds in a given two-week period
# x = Root = # of Root in a two week period

# Also, remember the "8 steps of Data Exploration", check for:
# 1. Outliers Y and X
# 2. Homogeneity Y
# 3. Normality Y
# 4. Zero trouble Y
# 5. Collinearity X('s)
# 6. Relationships Y and X
# 7. Interactions X('s)
# 8. Independence Y

# We will skip this part also...but do this for every dataset you work with as well

```

# Fit linear model to data (X as continuous variable)
```{r}
fit <- lm(Seed ~ Root, data=mydata)

# Extract predicted and residuals values
mydata$predicted <- predict(fit)
mydata$residuals <- residuals(fit)

# Basic scatterplot
a <- ggplot(mydata, aes(x = Root, y = Seed)) +
  geom_point() + theme_bw() 
a

# Add a lm

a + geom_smooth(method="lm", se=FALSE, color="darkgrey")

# Without lm, but with predicted values 

a + geom_point(aes(y=predicted), shape=1, color="blue")

# With predicted values + residuals

a + geom_point(aes(y=predicted), shape=1, color="blue") +
  geom_segment(aes(xend=Root, yend=predicted), alpha=0.5, color="red")

# With line and residuals

a + geom_segment(aes(xend=Root, yend=predicted), alpha=0.5, color="red") +
  geom_smooth(method="lm", se=FALSE, color="blue")

```

# Model diagnostics & output exploration
```{r}
# Explore model diagnostics
# We look for homocedasticity and normality in residuals 
par(mfrow = c(2,2)) #this tells R to set up a 2x2 grid of 4 plots
plot(fit) #create the diagnostic plots
par(mfrow = c(1,1)) #turn off the 4x4 grid setup or all your future plots will go onto a grid

# Explore residuals distribution with a histogram
hist(residuals(fit), breaks=10)
# Residuals are normal-ish distributed & homocedasticity looks OK

# Let's look at model output
summary(fit)

```

# >>>>>> Model output interpretation <<<<<<
Describe here what is the model output interpretation obtained above, recall differences between magnitude of effect and strenght of evidence of model.

Some ideas:
Beta[0] = -41.29; intercept of model fit crosses Root 0 at -41.29 seeds produced, which is not biologically possible, this would need some model adjustment, we will see later 
Beta[1] = 14.02; for every unit in root width there will be 14.02 more seeds produced
p-value of coefficients = Root width has an effect on Seed production at >95% confidence level
R-squared = Root width explains at least 70% of the variance of Seed production
p-value of model = model has >95% confidence level to reject null hypothesis

# Model visualization
```{r}
# Visualize model & data points
a <- ggplot(data=mydata, aes(x=Root, y=Seed)) +
  theme_bw() +
  geom_point() +
  geom_smooth(method="lm", se=FALSE, color="darkgrey") 
a

# Add CIs
# First estimate them based on our fitted model
CIs <- predict(fit, interval="confidence", level = 0.95)

lwr <- CIs[,2] # Lower CI
upr <- CIs[,3] # Upper CI

# Option 1: ggplot2 calculates CIs
a + geom_smooth(method=lm, se=TRUE) #se = confidence interval

# Option 2: Use our CI's estimated above
a + geom_line(aes(y=upr, x=mydata$Root), col="red") + 
  geom_line(aes(y=lwr, x=mydata$Root), col="red") 

# You could also add the regression equation on the plot
# Plot model, points, and an equation for the line
a <- ggplot(mydata, aes(x = Root, y = Seed)) + 
  geom_point() +
  theme_bw() +
  geom_smooth(method="lm", se=FALSE, color="darkgrey") +
  geom_line(aes(y=upr, x=mydata$Root), col="red") + 
  geom_line(aes(y=lwr, x=mydata$Root), col="red") +
  geom_text(x = 8, y = 23, label=lm_eqn(fit), parse=TRUE, size=6) 
a
```

# Model simulation
Here we will only estimate or predict datapoints based on our fitted model, this will help to illustrate the "inner workings of linear models". This next code estimates a normal distribution around 12 datapoints, each of these distributions is estimated based on the parameters of our fitted model. Made only for illustrative purposes, you do not need to run this for every model
```{r}
# Simple visualization of the model
plot(Seed ~ Root, 
      data=mydata, pch=16,
     ylim=c(-70, 160),
     xlim=c(-2, 12))
abline(fit, col="darkgrey", lwd=2)


# Simulate predicted values and add to plot
md <- seq(0, 11, length = 15)
Beta <- coef(fit)
for (i in 1:15){
  mu <- Beta[1] + Beta[2] * md[i]
  yi <- rnorm(50, mean = mu, sd = summary(fit)$sigma)
  points(x = jitter(rep(md[i], 50)), 
         y = jitter(yi), 
         col = grey(0.5), 
         pch = 16, 
         cex = 1)
}
abline(h = 0, lty=2)

```

# Fit Linear Model to data (X as categorical variable)
Now let's turn Root into a categorical variable, we could for example create group levels based on the number of Root
```{r}
# We can use the median to divide our categorical variable into two groups
median(mydata$Root) 

mydata2 <- mydata %>%
  mutate(widthroot = ifelse(Root < 7, "Level 1", "Level 2"))

mydata2$widthroot <- as.factor(mydata2$widthroot) # We need these variable as a factor

plot(Seed ~ widthroot, data=mydata2)#, xlab="Number of storms (categorical)")

categorical_lm1 <- lm(Seed ~ widthroot, data=mydata2)
summary(categorical_lm1)

# This is just like running a t-test
t.test(mydata2$Seed ~ mydata2$widthroot)

#-------------------------------------------#
# We can further divide our categorical variable into three or more groups

mydata3 <- mydata %>%
  mutate(widthroot = ifelse(Root < 5, "Level 1 (<5)", 
                            ifelse(Root < 13, "Level 2 (6-12)", "Level 3 (14-23)")))

mydata3$widthroot <- as.factor(mydata3$widthroot)

plot(Seed~widthroot, 
     data=mydata3, xlab="Number of storms (categorical)")

categorical_lm <- lm(Seed ~ widthroot, data=mydata3)

summary(categorical_lm)

# This is just like running an ANOVA
anova_version <- aov(Seed ~ widthroot, data=mydata3)
summary(anova_version)

```

